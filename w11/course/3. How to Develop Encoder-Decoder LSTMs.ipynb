{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition (Sum) Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from math import ceil\n",
    "from math import log10\n",
    "from numpy import argmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Generate Sum Pairs\n",
    "The first step is to generate sequences of random integers and their sum. We can put this in a function named random sum pairs(), as follows.\n",
    "\n",
    "1. n terms: The number of terms in the equation, (e.g. 2 for 10+10).\n",
    "2. largest: The largest numerical value for each term (e.g. 10 for values between 1-10).\n",
    "3. alphabet: The symbols used to encode the input and output sequences (e.g. 0-9, + and ‘ ’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate lists of random integers and their sum\n",
    "def random_sum_pairs(n_examples, n_numbers, largest):\n",
    "  X, y = list(), list()\n",
    "  for _ in range(n_examples):\n",
    "    in_pattern = [randint(1,largest) for _ in range(n_numbers)]\n",
    "    out_pattern = sum(in_pattern)\n",
    "    X.append(in_pattern)\n",
    "    y.append(out_pattern)\n",
    "    return X,y          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 8]] [11]\n"
     ]
    }
   ],
   "source": [
    "#------------For Seeing the output--------# \n",
    "n_samples = 1\n",
    "n_numbers = 2\n",
    "largest = 10\n",
    "# generate pairs\n",
    "X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "print(X, y)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Integers to Padded Strings\n",
    "\n",
    "The next step is to convert the integers to strings. The input string will have the format ‘10+10’ and the output string will have the format ‘20’. \n",
    "\n",
    "Key to this function is the padding of numbers to ensure that each input and output sequence has the same number of characters. A padding character should be different from the data so the model can learn to ignore them. \n",
    "\n",
    "In this case, we use the space character for padding(‘ ’) and pad the string on the left, keeping the information on the far right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to strings\n",
    "def to_string(X, y, n_numbers, largest):\n",
    "    max_length = 5\n",
    "    Xstr = list()\n",
    "    for pattern in X:\n",
    "        strp =  '+' .join([str(n) for n in pattern])\n",
    "        strp =   ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "        Xstr.append(strp)\n",
    "    max_length = int(ceil(log10(n_numbers * (largest+1))))\n",
    "    ystr = list()\n",
    "    for pattern in y:\n",
    "        strp = str(pattern)\n",
    "        strp =   ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "        ystr.append(strp)\n",
    "    return Xstr, ystr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 9]] [17]\n",
      "['  8+9'] ['17']\n"
     ]
    }
   ],
   "source": [
    "#Example of output of converting a sequence pair to padded characters.\n",
    "n_samples = 1\n",
    "n_numbers = 2\n",
    "largest = 10\n",
    "# generate pairs\n",
    "X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "print(X, y)\n",
    "# convert to strings\n",
    "X, y = to_string(X, y, n_numbers, largest)\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Integer Encoded Sequences\n",
    "\n",
    "Next, we need to encode each character in the string as an integer value. \n",
    "\n",
    "We have to work with numbers in neural networks after all, not characters. \n",
    "\n",
    "Integer encoding transforms the problem into a classification problem where the output sequence may be considered class outputs with 11 possible values each. \n",
    "\n",
    "To perform this encoding, we must define the full alphabet of symbols that may appear in the string encoding, as follows: alphabet=['0','1','2','3','4','5','6','7','8','9','+','']\n",
    "\n",
    "<b>Integer encoding then becomes a simple process of building a lookup table of character-to- integer offset and converting each char of each string, one by one. </b>\n",
    "The example below provides the <b>integer_encode()</b> function for integer encoding and demonstrates how to use it.\n",
    "\n",
    "<img src='intenc.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode strings\n",
    "def integer_encode(X, y, alphabet):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    Xenc = list()\n",
    "    for pattern in X:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        Xenc.append(integer_encoded)\n",
    "    yenc = list()\n",
    "    for pattern in y:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        yenc.append(integer_encoded)\n",
    "    return Xenc, yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 5]] [8]\n",
      "['  3+5'] [' 8']\n",
      "[[11, 11, 3, 10, 5]] [[11, 8]]\n"
     ]
    }
   ],
   "source": [
    "#Example of integer encoding padded sequences.\n",
    "n_samples = 1\n",
    "n_numbers = 2\n",
    "largest = 10\n",
    "# generate pairs\n",
    "X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "print(X, y)\n",
    "# convert to strings\n",
    "X, y = to_string(X, y, n_numbers, largest)\n",
    "print(X, y)\n",
    "# integer encode \n",
    "alphabet=['0','1','2','3','4','5','6','7','8','9','+',' '] \n",
    "X, y = integer_encode(X, y, alphabet)\n",
    "print(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 One Hot Encoded Sequences\n",
    "\n",
    "The next step is to binary encode the integer encoding sequences. This involves converting each integer to a binary vector with the same length as the alphabet and marking the specific integer with a 1. \n",
    "\n",
    "For example, a 0 integer represents the ‘0’ character and would be encoded as a binary vector with a 1 in the 0th position of an 11 element vector: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]. \n",
    "\n",
    "The example below defines the <b>one_hot_encode()</b> function for binary encoding and demonstrates how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode\n",
    "def one_hot_encode(X, y, max_int):\n",
    "    Xenc = list()\n",
    "    for seq in X:\n",
    "        pattern = list()\n",
    "    for index in seq:\n",
    "        vector = [0 for _ in range(max_int)]\n",
    "        vector[index] = 1\n",
    "        pattern.append(vector)\n",
    "    Xenc.append(pattern)\n",
    "    yenc = list()\n",
    "    for seq in y:\n",
    "        pattern = list()\n",
    "    for index in seq:\n",
    "         vector = [0 for _ in range(max_int)]\n",
    "         vector[index] = 1\n",
    "         pattern.append(vector)\n",
    "    yenc.append(pattern)\n",
    "    return Xenc, yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1]] [6]\n",
      "['  5+1'] [' 6']\n",
      "[[11, 11, 5, 10, 1]] [[11, 6]]\n",
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]] [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "#Example of one hot encoding an integer encoded sequences.\n",
    "n_samples = 1\n",
    "n_numbers = 2\n",
    "largest = 10\n",
    "# generate pairs\n",
    "X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "print(X, y)\n",
    "# convert to strings\n",
    "X, y = to_string(X, y, n_numbers, largest)\n",
    "print(X, y)\n",
    "# integer encode \n",
    "alphabet=['0','1','2','3','4','5','6','7','8','9','+',' '] \n",
    "X, y = integer_encode(X, y, alphabet)\n",
    "print(X, y)\n",
    "# one hot encode\n",
    "X, y = one_hot_encode(X, y, len(alphabet))\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Sequence Generation Pipeline\n",
    "\n",
    "We can tie all of these steps together into a function called generate data(), listed below. Given a designed number of samples, number of terms, the largest value of each term, and the alphabet of possible characters, the function will generate a set of input and output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for generating a sequence, encoding and reshaping it for an LSTM model.\n",
    "#Generate an encoded dataset\n",
    "def generate_data(n_samples, n_numbers, largest, alphabet):\n",
    "    # generate pairs\n",
    "    X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "    # convert to strings\n",
    "    X, y = to_string(X, y, n_numbers, largest)\n",
    "    # integer encode\n",
    "    X, y = integer_encode(X, y, alphabet)\n",
    "    # one hot encode\n",
    "    X, y = one_hot_encode(X, y, len(alphabet))\n",
    "    # return as NumPy arrays\n",
    "    X, y = array(X), array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Decode Sequences\n",
    "\n",
    "Finally, we need to invert the encoding to convert the output vectors back into numbers so we can compare expected output integers to predicted integers. The invert() function below performs this operation. Key is first converting the binary encoding back into an integer using the argmax() function, then converting the integer back into a character using a reverse mapping of the integers to chars from the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for deciding an encoded input or output sequence.\n",
    "#Invert encoding\n",
    "def invert(seq, alphabet):\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    strings = list()\n",
    "    for pattern in seq:\n",
    "        string = int_to_char[argmax(pattern)]\n",
    "        strings.append(string)\n",
    "    return  ''.join(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The network needs three configuration values defined by the specification of the addition problem:\n",
    "1. n terms: The number of terms in the equation, (e.g. 2 for 10+10).\n",
    "2. largest: The largest numerical value for each term (e.g. 10 for values between 1-10).\n",
    "3. alphabet: The symbols used to encode the input and output sequences (e.g. 0-9, + and ‘ ’)\n",
    "\n",
    "4. n chars: The size of the alphabet for a single time step (e.g. 12 for 0-9, ‘+’ and ‘ ’).\n",
    "5. n in seq length: The number of time steps of encoded input sequences (e.g. 8 for\n",
    "‘10+10+10’).\n",
    "6. n out seq length: The number of time steps of an encoded output sequence (e.g. 2 for ‘30’)\n",
    "\n",
    "We are now ready to define the Encoder-Decoder LSTM. We will use a single LSTM layer for the encoder and another single layer for the decoder.\n",
    "\n",
    "The encoder is defined with 75 memory cells and the decoder with 50 memory cells. \n",
    "\n",
    "The output layer uses the categorical log loss for the 12 possible classes that may be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of math terms\n",
    "n_terms = 2\n",
    "# largest value for any single input digit\n",
    "largest = 10\n",
    "# scope of possible symbols for each input or output time step\n",
    "alphabet = [str(x) for x in range(10)] + ['+', ' ']\n",
    "# size of alphabet: (12 for 0-9, + and    )\n",
    "n_chars = len(alphabet)\n",
    "# length of encoded input sequence (5 for 2 terms  10+10)\n",
    "n_in_seq_length = 5\n",
    "# length of encoded output sequence (2 for  30 )\n",
    "n_out_seq_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 75)                26400     \n",
      "_________________________________________________________________\n",
      "repeat_vector_10 (RepeatVect (None, 2, 75)             0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 2, 50)             25200     \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 2, 12)             612       \n",
      "=================================================================\n",
      "Total params: 52,212\n",
      "Trainable params: 52,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.4811 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.4638 - accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4467 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4294 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4118 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3934 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.3740 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.3533 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3310 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3066 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.2799 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.2504 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2177 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1811 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.1402 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0942 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.0423 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9836 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9171 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8417 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7563 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6599 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5517 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4313 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2994 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1579 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0101 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8615 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7179 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5850 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4661 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3630 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2759 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2047 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1487 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1064 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0755 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0537 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0385 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0011 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.9552e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.8916e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.8309e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.7727e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.7160e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.6629e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.6103e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.5599e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.5106e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.4625e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.4165e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.3706e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.3267e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f2ea1d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(n_in_seq_length, n_chars)))\n",
    "model.add(RepeatVector(n_out_seq_length))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_chars, activation= 'softmax' )))\n",
    "model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])\n",
    "model.summary()\n",
    "# fit LSTM\n",
    "X, y = generate_data(75000, n_terms, largest, alphabet)\n",
    "model.fit(X, y, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x14f8f4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Loss: 6.431972, Accuracy: 50.000000  \n"
     ]
    }
   ],
   "source": [
    "# evaluate LSTM\n",
    "X, y = generate_data(100, n_terms, largest, alphabet)\n",
    "loss, acc = model.evaluate(X, y, verbose=0)\n",
    "print(\"Loss: %f, Accuracy: %f  \"% (loss, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5+7 = 11 (expect 12)\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "for _ in range(1):\n",
    "    # generate an input-output pair\n",
    "    X, y = generate_data(1, n_terms, largest, alphabet)\n",
    "    # make prediction\n",
    "    yhat = model.predict(X, verbose=0)\n",
    "    # decode input, expected and predicted\n",
    "    in_seq = invert(X[0], alphabet)\n",
    "    out_seq = invert(y[0], alphabet)\n",
    "    predicted = invert(yhat[0], alphabet)\n",
    "    print(\"%s = %s (expect %s)\"  % (in_seq, predicted, out_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

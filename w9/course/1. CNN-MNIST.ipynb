{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e81cb7",
   "metadata": {},
   "source": [
    "## MNIST Digit Classification\n",
    "- Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with very little data cleaning or preparation required. Each image is a 28 â‡¥ 28 pixel square (784 pixels total).\n",
    "- 60,000 images are used to train a model and a separate set of 10,000 images are used to test it.\n",
    "- It is a digit recognition task. As such there are 10 digits (0 to 9) or 10 classes to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "073c1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a76d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ad hoc mnist instances\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd8935d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e337d32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032a990",
   "metadata": {},
   "source": [
    "## 1. Baseline Model with Multilayer Perceptrons or Artificial Neural Network (ANN)\n",
    "- Do we really need a complex model like a convolutional neural network to get the best results with MNIST? \n",
    "- You can get good results using a very simple neural network model with a single hidden layer. \n",
    "- We will use this as a baseline for comparison to more complex convolutional neural network models.\n",
    "- The training dataset is structured as a 3-dimensional array of (instance, image width and image height). \n",
    "- For a Multilayer Perceptron model we must reduce the images down into a vector of pixels. \n",
    "- In this case the 28 x 28 sized images will be 784 pixel input vectors. \n",
    "- We can do this transform easily using the reshape() function on the NumPy array.\n",
    "\n",
    "<img src='ann.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "261735d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "106d5f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "653e9546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "num_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3babda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f22b80db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5634c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69679e97",
   "metadata": {},
   "source": [
    "The pixel values are grayscale between 0 and 255. It is almost always a good idea to perform some scaling of input values when using neural network models. Because the scale is well known and well behaved, we can very quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d172987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cb2a7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "640d67d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b885f18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = y_test.shape[1]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56f7bb",
   "metadata": {},
   "source": [
    "## We are now ready to create our simple neural network model.\n",
    "- The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). \n",
    "- A rectifier activation function is used for the neurons in the hidden layer. \n",
    "- A softmax activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the modelâ€™s output prediction. \n",
    "\n",
    "<img src='1.png' width=150 height=150>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13043ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ea0f028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "210/210 [==============================] - 3s 8ms/step - loss: 1.1502 - accuracy: 0.6802 - val_loss: 0.3491 - val_accuracy: 0.9033\n",
      "Epoch 2/10\n",
      "210/210 [==============================] - 2s 8ms/step - loss: 0.3227 - accuracy: 0.9119 - val_loss: 0.2732 - val_accuracy: 0.9236\n",
      "Epoch 3/10\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 0.2555 - accuracy: 0.9285 - val_loss: 0.2356 - val_accuracy: 0.9331\n",
      "Epoch 4/10\n",
      "210/210 [==============================] - 1s 4ms/step - loss: 0.2201 - accuracy: 0.9367 - val_loss: 0.2160 - val_accuracy: 0.9382\n",
      "Epoch 5/10\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1890 - accuracy: 0.9463 - val_loss: 0.2021 - val_accuracy: 0.9425\n",
      "Epoch 6/10\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1709 - accuracy: 0.9499 - val_loss: 0.1910 - val_accuracy: 0.9448\n",
      "Epoch 7/10\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1600 - accuracy: 0.9546 - val_loss: 0.1789 - val_accuracy: 0.9482\n",
      "Epoch 8/10\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1443 - accuracy: 0.9594 - val_loss: 0.1736 - val_accuracy: 0.9501\n",
      "Epoch 9/10\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1353 - accuracy: 0.9614 - val_loss: 0.1649 - val_accuracy: 0.9529\n",
      "Epoch 10/10\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.1210 - accuracy: 0.9655 - val_loss: 0.1641 - val_accuracy: 0.9528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15cc99320>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_split=0.3, epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5d7c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.42\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74652bc2",
   "metadata": {},
   "source": [
    "## Try another ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d32052b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5d402d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "329/329 [==============================] - 6s 15ms/step - loss: 0.6094 - accuracy: 0.8049 - val_loss: 0.2374 - val_accuracy: 0.9276\n",
      "Epoch 2/10\n",
      "329/329 [==============================] - 5s 15ms/step - loss: 0.1142 - accuracy: 0.9641 - val_loss: 0.2399 - val_accuracy: 0.9190\n",
      "Epoch 3/10\n",
      "329/329 [==============================] - 5s 14ms/step - loss: 0.0729 - accuracy: 0.9782 - val_loss: 0.1533 - val_accuracy: 0.9581\n",
      "Epoch 4/10\n",
      "329/329 [==============================] - 5s 16ms/step - loss: 0.0498 - accuracy: 0.9845 - val_loss: 0.1120 - val_accuracy: 0.9704\n",
      "Epoch 5/10\n",
      "329/329 [==============================] - 5s 16ms/step - loss: 0.0357 - accuracy: 0.9893 - val_loss: 0.1195 - val_accuracy: 0.9699\n",
      "Epoch 6/10\n",
      "329/329 [==============================] - 6s 20ms/step - loss: 0.0294 - accuracy: 0.9907 - val_loss: 0.1636 - val_accuracy: 0.9638\n",
      "Epoch 7/10\n",
      "329/329 [==============================] - 7s 22ms/step - loss: 0.0231 - accuracy: 0.9935 - val_loss: 0.1278 - val_accuracy: 0.9738\n",
      "Epoch 8/10\n",
      "329/329 [==============================] - 8s 25ms/step - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.1300 - val_accuracy: 0.9757\n",
      "Epoch 9/10\n",
      "329/329 [==============================] - 8s 25ms/step - loss: 0.0160 - accuracy: 0.9952 - val_loss: 0.1506 - val_accuracy: 0.9732\n",
      "Epoch 10/10\n",
      "329/329 [==============================] - 9s 29ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.1533 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14be1cfd0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "245bf567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1064 - accuracy: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9811000227928162"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(X_test, y_test)[1]\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7575da2",
   "metadata": {},
   "source": [
    "## 2. Simple Convolutional Neural Network for MNIST\n",
    "- Next we need to load the MNIST dataset and reshape it so that it is suitable for use training a CNN. \n",
    "- In Keras, <strong> the layers used for two-dimensional convolutions expect pixel values with the dimensions [samples][width][height][channels]. </strong>\n",
    "- Note, we are forcing so-called channels-last ordering for consistency in this example. \n",
    "- <strong> In the case of RGB, the last dimension channels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. </strong>\n",
    "- <strong> In the case of MNIST where the channels values are grayscale, the pixel dimension is set to 1. </strong>\n",
    "\n",
    "<img src='cnn.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72064d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d5060df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d7ec8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638bb0d",
   "metadata": {},
   "source": [
    "Next we define our neural network model. Convolutional neural networks are more complex than standard Multilayer Perceptrons, so we will start by using a simple structure to begin with that uses all of the elements for state-of-the-art results. \n",
    "<br>Below summarizes the network architecture.\n",
    "- 1. The first hidden layer is a convolutional layer called a Conv2D. <br> The layer has 32 feature maps, with the size of 5 x 5 and a rectifier activation function.  <br> This is the input layer, expecting images with the structure outline above.\n",
    "- 2. Next we define a pooling layer that takes the maximum value called MaxPooling2D. <br> It is configured with a pool size of 2 x 2.\n",
    "- 3. The next layer is a regularization layer using dropout called Dropout. <br> It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    "- 4. Next is a layer that converts the 2D matrix data to a vector called Flatten. <br> It allows the output to be processed by standard fully connected layers.\n",
    "- 5. Next a fully connected layer with 128 neurons and rectifier activation function is used.\n",
    "- 6. Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.\n",
    "\n",
    "<img src='2.png' width=200 height=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "354ec3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc04737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 42s 133ms/step - loss: 0.4876 - accuracy: 0.8612 - val_loss: 0.0764 - val_accuracy: 0.9761\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 28s 93ms/step - loss: 0.0749 - accuracy: 0.9780 - val_loss: 0.0492 - val_accuracy: 0.9841\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 28s 93ms/step - loss: 0.0480 - accuracy: 0.9854 - val_loss: 0.0437 - val_accuracy: 0.9860\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 29s 96ms/step - loss: 0.0387 - accuracy: 0.9881 - val_loss: 0.0375 - val_accuracy: 0.9881\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 24s 79ms/step - loss: 0.0321 - accuracy: 0.9896 - val_loss: 0.0368 - val_accuracy: 0.9872\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 26s 87ms/step - loss: 0.0250 - accuracy: 0.9920 - val_loss: 0.0347 - val_accuracy: 0.9885\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 28s 93ms/step - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.0379 - val_accuracy: 0.9875\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 25s 83ms/step - loss: 0.0193 - accuracy: 0.9936 - val_loss: 0.0342 - val_accuracy: 0.9887\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 25s 85ms/step - loss: 0.0142 - accuracy: 0.9956 - val_loss: 0.0338 - val_accuracy: 0.9888\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 28s 92ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.0369 - val_accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15d0eb978>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29baa1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.82\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc22af9",
   "metadata": {},
   "source": [
    "## 3. Larger Convolutional Neural Network for MNIST\n",
    "- This time we define a larger CNN architecture with additional convolutional, max pooling layers and fully connected layers. The network topology can be summarized as follows.\n",
    "<br> 1. Convolutional layer with 30 feature maps of size 5 x 5. \n",
    "<br> 2. Pooling layer taking the max over 2 x 2 patches.\n",
    "<br> 3. Convolutional layer with 15 feature maps of size 3 x 3. \n",
    "<br> 4. Pooling layer taking the max over 2 x 2 patches.\n",
    "<br> 5. Dropout layer with a probability of 20%.\n",
    "<br> 6. Flatten layer.\n",
    "<br> 7. Fully connected layer with 128 neurons and rectifier activation. \n",
    "<br> 8. Fully connected layer with 50 neurons and rectifier activation. \n",
    "<br> 9. Output layer.\n",
    "\n",
    "<img src='3.png' width=200 height=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce94325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 34s 111ms/step - loss: 0.8537 - accuracy: 0.7322 - val_loss: 0.0837 - val_accuracy: 0.9738\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 32s 107ms/step - loss: 0.1076 - accuracy: 0.9685 - val_loss: 0.0477 - val_accuracy: 0.9845\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 40s 132ms/step - loss: 0.0697 - accuracy: 0.9787 - val_loss: 0.0359 - val_accuracy: 0.9877\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 85s 284ms/step - loss: 0.0570 - accuracy: 0.9821 - val_loss: 0.0309 - val_accuracy: 0.9889\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 147s 492ms/step - loss: 0.0494 - accuracy: 0.9848 - val_loss: 0.0325 - val_accuracy: 0.9897\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 103s 344ms/step - loss: 0.0413 - accuracy: 0.9872 - val_loss: 0.0301 - val_accuracy: 0.9894\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 94s 313ms/step - loss: 0.0378 - accuracy: 0.9875 - val_loss: 0.0282 - val_accuracy: 0.9912\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 80s 266ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 0.0244 - val_accuracy: 0.9913\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 80s 265ms/step - loss: 0.0312 - accuracy: 0.9900 - val_loss: 0.0301 - val_accuracy: 0.9899\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 81s 270ms/step - loss: 0.0295 - accuracy: 0.9910 - val_loss: 0.0253 - val_accuracy: 0.9924\n",
      "Accuracy: 99.24\n"
     ]
    }
   ],
   "source": [
    "# Larger CNN for the MNIST Dataset\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define the larger model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation= 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(15, (3, 3), activation= 'relu' ))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation= 'relu' ))\n",
    "model.add(Dense(50, activation= 'relu' ))\n",
    "model.add(Dense(num_classes, activation= 'softmax' ))\n",
    "# Compile model\n",
    "model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd261e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
